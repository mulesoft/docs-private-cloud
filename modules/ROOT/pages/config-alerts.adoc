
= Configure Alerts for Anypoint Platform PCE
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]

Anypoint Platform Private Cloud Edition (Anypoint Platform PCE) provides built-in alerts that are triggered when a condition specified in any alert definition is detected.

Measurements are stored in https://prometheus.io/docs/introduction/overview/[Promethus] and read by https://prometheus.io/docs/alerting/alertmanager/[Alertmanager]. Alertmanager sends emails when an alert is triggered.

== Alert Definitions

The following table shows the alerts that are shipped by default:

[%header%autowidth.spread]
.Alert Definitions
|===
| Component   | Alert         | Description
| CPU      | High CPU usage     | Triggers a warning, when > 75% used, with > 90% used, triggers a critical error
| Memory | High memory usage     | Triggers a warning, when > 80% used, with > 90% used, triggers a critical error
| Systemd      | Overall systemd health     |  Triggers an error when systemd detects a failed service
| Systemd      | Individual systemd unit health     | Triggers an error when a systemd unit is not loaded/active
| Filesystem | High disk space usage | Triggers a warning, when > 80% used, with > 90% used, triggers a critical error
| Filesystem | High inode usage | Triggers a warning, when > 90% used, with > 95% used, triggers a critical error
| System | Uptime | Triggers a warning when a node's uptime is less than 5min
| System | Kernel parameters | Triggers an error if a parameter is not set. See [value matrix](requirements.md#kernel-module-matrix) for details.
| Etcd | Etcd instance health | Triggers an error when an Etcd master is down longer than 5min
| Etcd | Etcd latency check | Triggers a warning, when follower <-> leader latency exceeds 500ms, then an error when it exceeds 1s over a period of 1min
| Docker | Docker daemon health | Triggers an error when docker daemon is down
| Kubernetes | Kubernetes node readiness | Triggers an error when the node is not ready
|===

== Configure Alerts Delivery

To configure Alertmanager to send email alerts you need to create some gravity resources

1. Use the following spec replacing the values with your SMTP config and create a file named `smtp-config.yaml` inside gravity:

    
    kind: smtp
    version: v2
    metadata:
        name: smtp
    spec:
        host: <SMTP_HOST>
        port: <SMTP_PORT>
        username: <SMTP_USERNAME>
        password: <SMTP_PASSWORD>
    ---
    kind: alerttarget
    version: v2
    metadata:
        name: email-alerts
    spec:
        # email address of the alerts recipient
        email: <RECIPIENT_EMAIL>
    

2. Run `gravity resource create smtp-config.yaml`. You should see the following output:

    
    Created cluster SMTP configuration
    Created monitoring alert target "email-alerts"

3. Add a default router to Alertmanager config by running this one-liner inside gravity:

    kubectl get secret -n monitoring alertmanager-main -o json | jq --arg foo "$(kubectl get secret -n monitoring alertmanager-main -o json | jq -r '.data["alertmanager.yaml"]' | base64 -d | yq r - --tojson | jq -r '.route.routes[1] |= . + {"match":{"alertname": "Watchdog", "receiver": "default", "continue": true}}' | jq -r '.route.routes[0].match += {"continue":true}' | yq r - -P | base64 | tr -d '\n')" '.data["alertmanager.yaml"]=$foo' | kubectl apply -f -

4. Configure the FROM email address by replacing <SMTP_FROM> in the next one-line and running it inside gravity:

    kubectl get secret -n monitoring alertmanager-main -o json | jq --arg foo "$(kubectl get secret -n monitoring alertmanager-main -o json | jq -r '.data["alertmanager.yaml"]' | base64 -d | yq w - 'global.smtp_from' <SMTP_FROM> | base64 | tr -d '\n')" '.data["alertmanager.yaml"]=$foo' | kubectl apply -f -

5. Restart alertmanager pods by running the following:

    kubectl delete pod -n monitoring -l app=alertmanager

6. Test Alertmanager by running the following inside gravity:

    curl -H 'Content-Type: application/json' -d '[{"labels":{"alertname":"test-alert","state":"firing"}}]' http://alertmanager-main.monitoring.svc.cluster.local:9093/api/v1/alerts

== Configure Alert Definitions 

- Defining new alerts is done via a gravity resource called alert:

    kind: alert
    version: v2
    metadata:
    name: cpu-alert
    spec:
    # the alert name
    alert_name: CPUAlert
    # the rule group the alert belongs to
    group_name: test-group
    # the alert expression
    formula: |
        node:cluster_cpu_utilization:ratio * 100 > 80
    # the alert labels
    labels:
        severity: info
    # the alert annotations
    annotations:
        description: |
        Cluster CPU usage exceeds 80%.

See https://prometheus.io/docs/prometheus/latest/configuration/alerting_rules/[Alerting Rules] documentation for more details about Prometheus alerts.

- Create the alert:

    gravity resource create alert.yaml

- View existing alerts:

    gravity resource get alerts

- Remove an alert:

    gravity resource rm alert cpu-alert

== Troubleshooting

** Verify that your SMTP server can send and receive emails using the addresses you configure as the `FROM` and `TO` addresses defined above
** Verify that your cluster nodes can communicate with your SMTP server.
+
For example, use `telnet` to connect to your SMTP server from one of your cluster nodes:
+

    telnet my.smtp.server.com 587
    Trying XXX.XXX.XXX.XXX...
    Connected to my.smtp.server.com.
    Escape character is '^]'.
    220 my.smtp.server.com ESMTP
    ^[^]
    telnet> quit
    Connection closed.

== See Also

* xref:managing-via-the-ops-center.adoc[Ops Center]
