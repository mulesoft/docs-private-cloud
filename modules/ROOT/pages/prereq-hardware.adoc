= Disk and Device Requirements for Anypoint Private Cloud
ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]
:page-aliases: prereq-verify-disk.adoc, prereq-verify-device.adoc
// Moved https://docs.mulesoft.com/private-cloud/2.0/prereq-verify-disk information here.
// Moved https://docs.mulesoft.com/private-cloud/2.0/prereq-verify-device information here.

To ensure the performance and stability of Anypoint Platform Private Cloud Edition, every node in your Anypoint Platform Private Cloud Edition environment must meet the requirements described in this topic. 

[IMPORTANT]
Before installing or upgrading,  your infrastructure team must review each of the following sections and verify that your environment meets the stated requirements. If needed, contact your Customer Success Manager for assistance.

== Memory and CPU Requirements

[%header%autowidth.spread]
|===
| Component |Requirement
|RAM |32 GB
|CPU |8 Cores
|===

== Disk Requirements

Anypoint Platform Private Cloud Edition has strict minimum size and performance requirements for storage devices used by the platform. All disk drives must meet the following criteria:

* Must have at least 300 MB/sec of throughput.
* Must have performance capabilities that can accomodate the expected environment traffic, even if they are virtual block devices or network attached storage.

=== Verify Disk Throughput

On each node, measure disk throughput using a tool such as `hdparm`. On CentOS, `hdparm` can be installed by running the following command:

----
sudo yum install -y hdparm
----

Run the following command to verify disk throughput:

----
sudo hdparm -d <device>
----

For example:

----
$ sudo hdparm -t /dev/sdd

/dev/sdd:
Timing buffered disk reads: 4726 MB in  3.00 seconds = 1574.94 MB/sec
----

You can also measure throughput using the `dd`  tool if your devices are not bare devices. Do not use the `dd` tool on bare devices. The `dd` command provides less accurate information than `hdparm`, but is available on any operating system and provides the ability to easily verify general disk performance. `dd` writes directly to a specified file, even if it is a device. For bare devices, after the device is formatted and mounted, you can write to a file on that device to measure throughput, as shown in the following example:

----
$ sudo mount /dev/sdb /var/lib/gravity # must be already formatted!
$ sudo dd if=/dev/zero of=/var/lib/gravity/testfile count=1000 bs=1M
1000+0 records in
1000+0 records out
1048576000 bytes (1.0 GB, 1000 MiB) copied, 0.382535 s, 2.7 GB/s
----

== Device Requirements
For production environments, each node in your configuration must have the following dedicated devices. All devices must be formatted either as *xfs* or *ext4*.

[%header%autowidth.spread]
|===
| Component |Size|Minimum IOPS|Mount Point|Description
|HDD 1 (Operating system) |50 GB |500| |
|HDD 2 (System state directory) | 100 GB |1500|/var/lib/gravity | Stores system configuration and metadata, including databases and packages. As package sizes can be large, it is important to estimate the minimum size requirements and allocate enough space as a dedicated device before installation.
|HDD 3 (Application data)| 250 GB |1500|/var/lib/data | Stores application configuration and data. The amount of space required should be at least 250 GB, but might vary depending on your specific use case. It is important to estimate the minimum size requirements and allocate enough space as a dedicated device ahead of time.
|HDD 5 (Etcd) | 20 GB |3000|/var/lib/gravity/planet/etcd | Provides dedicated storage for the distributed database used for cluster coordination.
|HDD 4 (Docker) | 100 GB |3000|Not mounted | Used by Docker’s Device Mapper storage driver. A minimum of 100Gb is required for the Device Mapper directory. Devices with 50Gb or less will experience degraded system performance or might not work at all.
|/tmp (Installer) | 20 GB |N/A| |
|===

[NOTE]
The space requirements listed in the previous table are based on average use. For environments with heavy traffic, validate the amount of space needed with your Customer Success Manager.

=== System State Directory Device

The main purpose of the system state directory device is to store system configuration and metadata - for example, database and packages. As package sizes can be arbitrary large, it is important to estimate the minimum size requirements and allocate enough space for the dedicated device.

This device is formatted either as `xfs` or `ext4` and mounted as `/var/lib/gravity`.  

The following shell snippet is provided as an example for mounting the system state directory device using systemd mount files and should not be used as-is in production. Mounting the system state directory device can also be achieved by using `/etc/fstab` or some other method. Consult with your system administrator to determine the best way to meet this requirement. If you use the following example to guide this process, make sure to specify the correct device name in two places.

----
sudo mkfs.ext4 /dev/<device name>
sudo mkdir -p /var/lib/gravity
echo -e "[Mount]\nWhat=/dev/<device name>\nWhere=/var/lib/gravity\nType=ext4\n[Install]\nWantedBy=local-fs.target" | sudo tee /etc/systemd/system/var-lib-gravity.mount
sudo systemctl daemon-reload
sudo systemctl enable var-lib-gravity.mount
sudo systemctl start var-lib-gravity.mount
----

=== Application Data Device

The main purpose of application data directory device is to store application configuration and data. The minimum amount of space required is 250GB, but your use case might require more. It is important to allocate enough space for the dedicated device.

This device is formatted either as `xfs` or `ext4` and mounted as `/var/lib/data`. 

The following shell snippet is provided as an example for mounting the application data device using systemd mount files, and should not be used as-is in production. Mounting the application data device can also be achieved  using `/etc/fstab` or some other method. Consult with your system administrator to determine the best way to meet this requirement. If you use the following example to guide this process, make sure to specify the correct device name in two places.

----
sudo mkfs.ext4 /dev/<device name>
sudo mkdir -p /var/lib/data
echo -e "[Mount]\nWhat=/dev/<device name>\nWhere=/var/lib/data\nType=ext4\n[Install]\nWantedBy=local-fs.target" | sudo tee /etc/systemd/system/var-lib-data.mount
sudo systemctl daemon-reload
sudo systemctl enable var-lib-data.mount
sudo systemctl start var-lib-data.mount
----

=== Etcd Device

The main purpose of the etcd device is to provide dedicated storage for a distributed database used for cluster coordination. It does not require much space, 20GB is adequate.

This device is formatted either as `xfs` or `ext4` and mounted as `/var/lib/gravity/planet/etcd`. 

The following shell snippet is provided as an example for mounting the etcd device using systemd mount files, and should not be used as-is in production. The end result of this step mounts the Etcd device to the path `/var/lib/gravityi/planet/etcd`. Mounting the etcd device can also be achieved using `/etc/fstab` or some other method. Consult with your system administrator to determine the best way to meet this requirement. If you use the following example to guide this process, make sure to specify the correct device name in two places.

----
sudo mkfs.ext4 /dev/<device name>
sudo mkdir -p /var/lib/gravity/planet/etcd
echo -e "[Mount]\nWhat=/dev/<device name>\nWhere=/var/lib/gravity/planet/etcd\nType=ext4\n[Install]\nWantedBy=local-fs.target" | sudo tee /etc/systemd/system/var-lib-gravity-planet-etcd.mount
sudo systemctl daemon-reload
sudo systemctl enable var-lib-gravity-planet-etcd.mount
sudo systemctl start var-lib-gravity-planet-etcd.mount
----

=== Docker Device

Unless specified, Docker configuration defaults to the use of Device Mapper in loopback mode (using `/dev/loopX` devices), which is not recommended for production. To configure Docker to use a dedicated device for Device Mapper storage driver, an unformatted device (or a partition) (i.e. /dev/sdd) can be provided during installation. This directory is automatically configured and set up for use.

Unformatted devices potentially usable for system directory / Device Mapper are automatically discovered by agents running on each node. Discovered devices are offered on a drop-down menu for configuration before the installation is started.

You can list unmounted devices with the following command:
----
lsblk --output=NAME,TYPE,SIZE,FSTYPE -P -I 8,9,202|grep 'FSTYPE=""'
----

Unmounted devices have an empty value in FSTYPE column. Devices with TYPE="part" are partitions on another device. This command only lists specific device types, as shown in the following example:

|===
|Device type|Description
|8   |SCSI disk devices
|9   |Metadisk (RAID) devices
|202 |Xen virtual block devices (Amazon EC2)
|===

== Verify Devices

To verify that all devices are set up the same way on all nodes, run the `lsblk` command. Output from `lsblk` should resemble the following example: 

----
[root@ip-0-0-0-0 ~]# lsblk
NAME MAJ:MIN RM SIZE RO TYPE MOUNTPOINT
xvda 202:0 0 90G 0 disk
├─xvda1 202:1 0 1M 0 part
└─xvda2 202:2 0 90G 0 part /
xvdb 202:16 0 100G 0 disk -- Docker volume (not formatted)
xvdc 202:32 0 20G 0 disk /var/lib/gravity/planet/etcd
xvdd 202:48 0 250G 0 disk /var/lib/data
xvde 202:64 0 100G 0 disk /var/lib/gravity
xvdf 202:64 0 20G 0 disk /tmp
----

=== Manually Reset Devices and Partitions

Using Logical Volume Manager, you can group multiple physical volumes into a single storage volume (Volume Group) and then divide these into Logical Volumes. Physical Volumes are either a whole device or a partition.

In some cases, when a device is in use by another logical volume or you want to manually reset a device previously configured for Device Mapper, the following Logical Volume Manager commands can be useful.

* dmsetup - is a low-level logical volume management
* pv/vg/lv-prefixed commands like pvdisplay and pvcreate/pvremove - for working with specific LVM object types (i.e. lv - for logical volumes and vg - for volume groups)

You can reset a device using the following steps:

* remove logical volume with `lvremove -f docker/thinpool` (use `lvdisplay` to find the volume to remove)
* remove volume group with `vgremove docker` (use `vgdisplay` to locate the volume group to remove)
* remove physical volume and reset device with `pvremove /dev/<device name>` (use `pvdisplay` to find the physical volume to remove and the device name it is on)

== Verify Disk IOPS

Depending on your hardware or virtualization provider, you can configure disk IOPS (I/O Operations Per Second). Using the `iops` command, verify available IOPS on each device:

----
$ sudo ./iops --time 2 /dev/xvdb
/dev/xvdb, 107.37 GB, 32 threads:
 512   B blocks: 1893.0 IO/s, 946.5 KiB/s (  7.8 Mbit/s)
   1 KiB blocks: 1354.8 IO/s,   1.3 MiB/s ( 11.1 Mbit/s)
   2 KiB blocks: 1091.8 IO/s,   2.1 MiB/s ( 17.9 Mbit/s)
   4 KiB blocks:  807.1 IO/s,   3.2 MiB/s ( 26.4 Mbit/s)
   8 KiB blocks:  803.7 IO/s,   6.3 MiB/s ( 52.7 Mbit/s)
  16 KiB blocks:  787.4 IO/s,  12.3 MiB/s (103.2 Mbit/s)
  32 KiB blocks:  700.8 IO/s,  21.9 MiB/s (183.7 Mbit/s)
  64 KiB blocks:  590.0 IO/s,  36.9 MiB/s (309.3 Mbit/s)
 128 KiB blocks:  327.6 IO/s,  40.9 MiB/s (343.5 Mbit/s)
...
----

== See Also

* xref:supported-cluster-config.adoc[Supported Network Topologies]
* xref:verify-nfs.adoc[Verify NFS Server Requirements]
* xref:install-create-lb.adoc[Set up a Load Balancer for Anypoint Platform Private Cloud Edition]
