= Anypoint Monitoring and Visualizer

ifndef::env-site,env-github[]
include::_attributes.adoc[]
endif::[]

== Overall Architecture

Anypoint Monitoring is an optional feature during installation, which includes Anypoint Visualizer. Both products require 3 dedicated nodes, normally labeled *amv_node*.

In those 3 nodes, the following Helm charts will be installed in 4 different namespaces:

|===
|Namespace | Service | Notes

.6+|dias
|prov-k8s-am-influxdb-comp
|Influx DB
|prov-k8s-am-ingestor-comp
|Logstash service
|prov-k8s-amr-certs-comp
|Service for certificates generation/rotation
|prov-k8s-amr-ingestor-router-comp
|Public Nginx entrypoint (proxy) for mule agent events
|prov-k8s-insight-comp
|Alerting engine
|provisioning-api
|Configuration service for all Monitoring/Visualizer

.6+|monitoring-center
|alerts-api
|Basic alerts functionality
|metrics-api
|Influx DB query service
|settings-api
|User settings service
|ui
|Nginx service for serving static UI artifact files
|ui-api
|Gateway / Experience API
|visualizer
|Grafana fork responsible for advanced alerts, dashboards

|default
|stolon-amv
|An additional Postgres 9.6 cluster for analytics products only

.5+|visualizer
|experience-api
|Gateway / Experience API
|janitor
|Contains a set of cron jobs for stale data clean up
|topology-api
|Building application network topology for Visualizer
|topology-processor
|Processing mule runtime events for Visualizer
|ui
|Nginx service for serving static UI artifact files

|===

=== DIAS Components

DIAS provides the foundations for data transmission between the Mule runtime (running Filebeat) and the InfluxDB storing that data. Once data reaches InfluxDB, it becomes available to the Anypoint Monitoring UI as well as the Insight alerting engine.

The `dias-prov-k8s-amr-ingestor-router-comp` pods run Nginx accepting connections from remote runtimes thought mTLS (it uses a node-port service). Data is then forwarded to `dias-prov-k8s-am-ingestor-comp` pods running Logstash.

Logstash processes and transform those messages and batch them before hitting InfluxDB. The `dias-prov-k8s-am-influxdb-comp` component is a stateful InfluxDB cluster consisting in 3 meta and 2 data nodes.

The Insight alerting engine is managed by the `dias-prov-k8s-insight-comp` component. It periodically fetches metrics from InfluxDB and based on the customer's alerts definition, the engine triggers alarms. Each pod has 4 containers, including a Mule application, Meld, Meld-Drill and Cassandra.

Logstash, Insight and InfluxDB are all K8s stateful sets.


=== Monitoring Center Components

Monitoring Center components represents the "Anypoint Monitoring" product that visualizer the data stored in DIAS products.

All user traffic goes to the experience api (`monitoring-center-ui-api`) and from there it can proxied to:

- `monitoring-center-settings-api` to manage user settings.

- `monitoring-center-alerts-api` to manage alerts, synchronize the changes with Insights engine and trigger basic alerts notifications.

- `monitoring-center-visualizer` that is Grafana fork. It manages the dashboards logic, advanced alerts and the rest of the functionality available in PCE 3.0.

- `monitoring-center-metrics-api` to validate Influx DB queries and proxy the calls to the data source (`dias-prov-k8s-am-influxdb-comp`).

=== Visualizer Components

Visualizer components represents the "Anypoint Visualizer" product.

Logstash `dias-prov-k8s-am-ingestor-comp` sends a subset of mule agent events to the `visualizer-topology-processor` service.
That service process those events and stores the results in the Postgres DB (`stolon-amv`).
The final application network generation will be handled by `visualizer-topology-api` service and is invoked by a user traffic.
There is a set of cron jobs to prevent the uncontrolled DB growth for mule agent events. They perform the stale data clean and runs as part of `visualizer-janitor` service.

All user traffic comes to the `visualizer-experience-api` service that orchestrates loading all necessary data from different data sources (like `visualizer-topology-api`, `monitoring-center-ui-api`, etc.) and performs further data decorations.

== Considerations

=== Cluster secrets

All Anypoint Monitoring/Visualizer components use self-signed certificates for TLS support. Those certificates are generated by the `dias-prov-k8s-amr-certs-comp` components and stored as secrets within the cluster under the `dias` namespace. It is important to notice if the customer changes the platform DNS, the `dias-prov-k8s-amr-certs-comp` will be redeployed and secrets will be updated. Additionally, the `dias-prov-k8s-amr-ingestor-router-comp` component, running Nginx, will be redeployed as well, refreshing those new secrets.

=== Stateful sets directories

Each stateful set uses different directories to hold its data, resulting in the following directories:

```
  /var/lib/data/influxdb/data
  /var/lib/data/influxdb/meta
  /var/lib/data/logstash
  /var/lib/data/dias-cassandra
  /var/lib/data/dias-meld
```

InfluxDB meta pods (3 pods) use `/var/lib/data/influxdb/meta` while data pods (2 pods) use `/var/lib/data/influxdb/data`. Logstash uses `/var/lib/data/logstash` and finally Insight uses the remaining two directories to hold its data.

== Requirements

Hardware requirements are listed in the main PCE hardware section but 3 nodes are required for all Anypoint Monitoring/Visualizer workload.

Port 8895 needs to be accesible in the main balancer along with the rest of the required ports.


=== InfluxDB License

InfluxDB requires a valid license to operate. Anypoint Monitoring requires a Kubernetes secret to be present in the cluster before all Anypoint Monitoring/Visualizer charts are deployed.

The following command needs to be run in order to create the required secret in the default namespace:


```
kubectl create secret generic influxdb-license --from-literal=license={license}
```

Note that InfluxDB license might be a JSON file in which case we only need the `key` value in that secret.

If no secret is given, InfluxDB will fail with the following message

```
ts=ZULU_TIME lvl=error msg="LICENSE IS INVALID OR CANNOT BE FOUND" log_id=LOG_ID service=licensing error=ERROR_MESSAGE
```

If that is the case, the secret will need to be created and InfluxDB chart redeployed. Before redeploying InfluxDB, all *pvc*, *pv* and *jobs* (named `dias-prov-k8s-am-influxdb-comp-bootstrap`) will need to be deleted.



== Restore/Backup

Both backup and restore procedures for Anypoint Monitoring are outside the normal Gravitational backup/restore mechanism and therefore requires additional procedures.

Anypoint Monitoring/Visualizer need to backup the following component's data

. InfluxDB (`dias-prov-k8s-am-influxdb-comp`)
. Cassandra (`dias-prov-k8s-am-ingestor-comp`)


Each component has its own way to backup, and restore, the data. It is important to mentioned that the Anypoint Monitoring/Visualizer custom Stolon DB will be backup/restore by the default Gravitational mechanism and it is not considered in this document.

---
It is up to the customer to provide enough space to run the backup/restore for each of the mentioned components, considering that we will need double the space, at most. For instance, InfluxDB data directories might use 200GB and therefore the underlying disk (where the backup is meant to be written) should have 200GB+ free space.
---

=== InfluxDB

All InfluxDB data/meta nodes run on the 3 nodes labeled `amv_node` and therefore we need to backup the data sitting in those 3 nodes. Data is under '/var/lib/data/influxdb' for both meta and data pods. The following steps consider that the underlying hard drive has enough space to hold the backup. The destination can be actually anything (disk, S3 bucket, etc).

. `kubectl get nodes | grep amv` will provide the Anypoint nodes
. Connect to each `amv` node and run `sudo gravity enter`
. Under `cd /var/lib/data/influxdb` we can find the data that needs to be backup. At this point, any approach to copy that data to a secure location is acceptable. The following steps are provided just as a local backup.
. Copy over data with `cp -a meta/. bck/meta & cp -a data/. bck/data` (`bck` is just a directory in the same location, it can be anywhere)
. On a fresh new installation with InfluxDB pods already running we will restore data with `cp -a bck/data/. data/ & cp -a bck/meta/. meta/`
. Restarting all meta/data pods will reload that data


Backing up InfluxDB is as simple as copying over all the data under `/var/lib/data/influxdb`. Upon restore, all influxDB pods need to be restarted (if they are already running) to restore the data.

=== Cassandra

Ejaz to complete

=== Logstash

When it comes to Logstash data (`dias-prov-k8s-am-ingestor-comp`) it is important to consider that Logstash keeps "watermark" files which indicate which metric files have been processed. In a fresh new instal (either from scratch or after a restore) there will not be any metric files to process and therefore no need to backup those "watermark" files. This is acceptable since Logstash has a flush mechanism built-in where upon pod graceful termination, all data is flushed to InfluxDB.
